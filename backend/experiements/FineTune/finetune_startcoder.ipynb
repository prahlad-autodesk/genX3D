{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1452aabb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dict, List, Optional\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     10\u001b[39m     AutoTokenizer,\n\u001b[32m     11\u001b[39m     AutoModelForCausalLM,\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m     default_data_collator,\n\u001b[32m     15\u001b[39m )\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpeft\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftConfig\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "# finetune_qlora_starcoder.py\n",
    "import os\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    default_data_collator,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftConfig\n",
    "\n",
    "# -------------------------\n",
    "# USER CONFIG\n",
    "# -------------------------\n",
    "MODEL_NAME = os.environ.get(\"MODEL_NAME\", \"bigcode/starcoder\")  # change if different\n",
    "DATA_PATH = os.environ.get(\"DATA_PATH\", \"data/prompts_codes.jsonl\")  # jsonl with {\"prompt\": \"...\", \"code\": \"...\"}\n",
    "OUTPUT_DIR = os.environ.get(\"OUTPUT_DIR\", \"outputs/qlora-starcoder\")\n",
    "BATCH_SIZE = int(os.environ.get(\"BATCH_SIZE\", \"8\"))\n",
    "EVAL_BATCH_SIZE = int(os.environ.get(\"EVAL_BATCH_SIZE\", \"4\"))\n",
    "LEARNING_RATE = float(os.environ.get(\"LR\", \"2e-4\"))\n",
    "NUM_EPOCHS = int(os.environ.get(\"EPOCHS\", \"3\"))\n",
    "MAX_LENGTH = int(os.environ.get(\"MAX_LEN\", \"2048\"))\n",
    "GRAD_ACCUM_STEPS = int(os.environ.get(\"GRAD_ACCUM\", \"1\"))\n",
    "\n",
    "# LoRA / QLoRA config\n",
    "LORA_R = int(os.environ.get(\"LORA_R\", \"16\"))\n",
    "LORA_ALPHA = int(os.environ.get(\"LORA_ALPHA\", \"32\"))\n",
    "LORA_DROPOUT = float(os.environ.get(\"LORA_DROPOUT\", \"0.05\"))\n",
    "# Target modules vary by model. For many causal transformers, q/k/v/o works.\n",
    "TARGET_MODULES = os.environ.get(\"TARGET_MODULES\", \"q_proj,k_proj,v_proj,o_proj\").split(\",\")\n",
    "\n",
    "# -------------------------\n",
    "# utility / dataset prep\n",
    "# -------------------------\n",
    "def build_prompt(prompt: str, code: str):\n",
    "    \"\"\"\n",
    "    Combine into a single text for causal LM.\n",
    "    You can change template to match how model was trained (instruction style, etc.)\n",
    "    \"\"\"\n",
    "    # Keep it simple: separate with sentinel\n",
    "    return f\"{prompt}\\n\\n### Code:\\n{code}\\n\"\n",
    "\n",
    "def tokenize_and_mask(examples, tokenizer, max_length=2048):\n",
    "    \"\"\"\n",
    "    For each example, we return:\n",
    "      - input_ids: tokenized(prompt + code)\n",
    "      - labels: same as input_ids but with prompt tokens set to -100 so loss is only on code\n",
    "    \"\"\"\n",
    "    prompts = examples[\"prompt\"]\n",
    "    codes = examples[\"code\"]\n",
    "    input_ids_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    for p, c in zip(prompts, codes):\n",
    "        combined = build_prompt(p, c)\n",
    "        # tokenize prompt alone to find length\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            # For causal models we use same tokenizer for prompt and target\n",
    "            prompt_tokens = tokenizer(p, add_special_tokens=False)[\"input_ids\"]\n",
    "        # tokenize combined\n",
    "        tokenized = tokenizer(\n",
    "            combined,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            padding=False,\n",
    "            return_tensors=None,\n",
    "            add_special_tokens=True,\n",
    "        )[\"input_ids\"]\n",
    "\n",
    "        # create labels: mask prompt part\n",
    "        prompt_len = len(prompt_tokens)\n",
    "        labels = tokenized.copy()\n",
    "        for i in range(min(prompt_len, len(labels))):\n",
    "            labels[i] = -100  # ignore prompt tokens in loss\n",
    "\n",
    "        input_ids_list.append(tokenized)\n",
    "        labels_list.append(labels)\n",
    "\n",
    "    return {\"input_ids\": input_ids_list, \"labels\": labels_list}\n",
    "\n",
    "# Custom collator to pad input_ids and labels the same way\n",
    "@dataclass\n",
    "class DataCollatorForCausal:\n",
    "    tokenizer: AutoTokenizer\n",
    "    padding: bool = True\n",
    "    max_length: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, List[int]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids = [f[\"input_ids\"] for f in features]\n",
    "        labels = [f[\"labels\"] for f in features]\n",
    "\n",
    "        batch = self.tokenizer.pad(\n",
    "            {\"input_ids\": input_ids, \"labels\": labels},\n",
    "            padding=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        # Ensure labels tensor exists and dtype long\n",
    "        batch[\"labels\"] = batch[\"labels\"].to(torch.long)\n",
    "        return batch\n",
    "\n",
    "# -------------------------\n",
    "# Main\n",
    "# -------------------------\n",
    "def main():\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    # 1) tokenizer and dataset\n",
    "    print(\"Loading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "    # Ensure tokenizer has pad token for padding (important)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({\"pad_token\": \"</s>\"})\n",
    "\n",
    "    print(\"Loading dataset...\")\n",
    "    # Accept either jsonl or huggingface dataset path\n",
    "    if DATA_PATH.endswith(\".jsonl\") or DATA_PATH.endswith(\".json\"):\n",
    "        dataset = load_dataset(\"json\", data_files={\"train\": DATA_PATH})\n",
    "    else:\n",
    "        # assume generic dataset path (Hugging Face)\n",
    "        dataset = load_dataset(DATA_PATH)\n",
    "\n",
    "    # If your dataset already has train/validation splits, adapt here\n",
    "    # For example: dataset = dataset[\"train\"].train_test_split(test_size=0.02)\n",
    "    if \"validation\" not in dataset:\n",
    "        dataset = dataset[\"train\"].train_test_split(test_size=0.02)\n",
    "        train_ds = dataset[\"train\"]\n",
    "        val_ds = dataset[\"test\"]\n",
    "    else:\n",
    "        train_ds = dataset[\"train\"]\n",
    "        val_ds = dataset[\"validation\"]\n",
    "\n",
    "    # Map tokenization with masking\n",
    "    print(\"Tokenizing and creating labels (masking prompt tokens)...\")\n",
    "    tokenized_train = train_ds.map(\n",
    "        lambda ex: tokenize_and_mask(ex, tokenizer, max_length=MAX_LENGTH),\n",
    "        batched=True,\n",
    "        remove_columns=train_ds.column_names,\n",
    "    )\n",
    "    tokenized_val = val_ds.map(\n",
    "        lambda ex: tokenize_and_mask(ex, tokenizer, max_length=MAX_LENGTH),\n",
    "        batched=True,\n",
    "        remove_columns=val_ds.column_names,\n",
    "    )\n",
    "\n",
    "    # 2) model loading in 4-bit (QLoRA preps)\n",
    "    print(\"Loading model in 4-bit mode...\")\n",
    "    # bitsandbytes 4-bit config via transformers `load_in_4bit`\n",
    "    bnb_kwargs = {\n",
    "        \"load_in_4bit\": True,\n",
    "        \"bnb_4bit_compute_dtype\": torch.float16,\n",
    "        \"bnb_4bit_use_double_quant\": True,\n",
    "        \"bnb_4bit_quant_type\": \"nf4\",  # nf4 is common for LLMs\n",
    "    }\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16,\n",
    "        trust_remote_code=False,\n",
    "        **bnb_kwargs,\n",
    "    )\n",
    "\n",
    "    # Prepare model for k-bit training (this enables gradient checkpointing etc.)\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    # 3) attach LoRA adapters (PEFT)\n",
    "    print(\"Applying LoRA...\")\n",
    "    lora_config = LoraConfig(\n",
    "        r=LORA_R,\n",
    "        lora_alpha=LORA_ALPHA,\n",
    "        target_modules=TARGET_MODULES,\n",
    "        lora_dropout=LORA_DROPOUT,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()  # logs % trainable params (should be small)\n",
    "\n",
    "    # 4) training args & trainer\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        fp16=True,\n",
    "        logging_steps=50,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=500,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=500,\n",
    "        save_total_limit=3,\n",
    "        gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
    "        weight_decay=0.01,\n",
    "        dataloader_num_workers=4,\n",
    "        optim=\"paged_adamw_32bit\",  # works with bitsandbytes + huge models\n",
    "        report_to=\"none\",  # change to \"wandb\" if using Weights & Biases\n",
    "        remove_unused_columns=False,\n",
    "        push_to_hub=False,\n",
    "    )\n",
    "\n",
    "    data_collator = DataCollatorForCausal(tokenizer=tokenizer, max_length=MAX_LENGTH)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_val,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    # 5) train\n",
    "    print(\"Starting training...\")\n",
    "    trainer.train()\n",
    "    print(\"Training finished.\")\n",
    "\n",
    "    # 6) save adapter weights (LoRA) so base model remains untouched\n",
    "    print(\"Saving LoRA adapters to\", OUTPUT_DIR)\n",
    "    model.save_pretrained(OUTPUT_DIR)\n",
    "    tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "    print(\"Done. Adapter + tokenizer saved.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975dfe99",
   "metadata": {},
   "source": [
    "# setup instruction and observations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687fadcf",
   "metadata": {},
   "source": [
    "Nice — below is a ready-to-run, well-commented QLoRA fine-tuning script for a causal code model like **StarCoder** (you wrote \"start coder\" — I assume you meant **StarCoder**; if it's a different checkpoint, just point `MODEL_NAME` at it). The script trains on a dataset of `{ \"prompt\": \"...\", \"code\": \"...\" }` JSONL entries and **only computes loss on the code** part (prompt tokens are masked with `-100`).\n",
    "\n",
    "### Quick notes before the script\n",
    "\n",
    "* QLoRA uses 4-bit quantization (bitsandbytes) + PEFT (LoRA). You need a GPU with sufficient memory; best with A100/RTX40x0, or use multiple GPUs (Accelerate).\n",
    "* Installs: `transformers`, `datasets`, `accelerate`, `peft`, `bitsandbytes`, `torch`, plus optionally `sentencepiece`/`tokenizers` if needed.\n",
    "* This script uses `transformers.Trainer` for simplicity. For very large setups prefer Hugging Face `accelerate` low-level or `trlx`.\n",
    "* **Adjust** `TARGET_MODULES` to modules used by your model architecture (LoRA target modules vary across models).\n",
    "* Save only the LoRA adapter (small) — keep base checkpoint separate.\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "## Tips & tuning\n",
    "\n",
    "* **Target modules**: If you see warnings or no parameter changes, inspect `model.named_modules()` and choose correct linear layer names (common: `q_proj`, `k_proj`, `v_proj`, `o_proj`, `fc1`, `fc2`, `gate_proj` etc.). Use `print([n for n, m in model.named_modules() if isinstance(m, torch.nn.Linear)])` to inspect.\n",
    "* **Batch size**: increase via `gradient_accumulation_steps` if GPU memory limited.\n",
    "* **Optimizer**: `paged_adamw_32bit` generally works well with large models + bitsandbytes. If not available, try `adamw_torch`.\n",
    "* **Eval metric**: For code models you may want to compute exact-match or BLEU on code and also run syntax checks / execution tests after save.\n",
    "* **Tokenizer**: Make sure the tokenizer’s special tokens match model expectations (e.g., `bos_token`, `eos_token`). Add them if missing.\n",
    "* **Long contexts**: For very long code, increase `MAX_LENGTH` and ensure vLLM/SageMaker inference supports it.\n",
    "* **Validation**: Use small eval set to ensure loss decreases and generate samples periodically.\n",
    "\n",
    "---\n",
    "\n",
    "If you want, I can:\n",
    "\n",
    "* Generate a ready-to-run `accelerate` config + `launch` example for multi-GPU training.\n",
    "* Produce a sample `hf_dataset` JSONL toy file and a small test run.\n",
    "* Tailor the `TARGET_MODULES` after I inspect `model.named_modules()` from your exact StarCoder checkpoint. Which model checkpoint are you using (HF Hub name or local path)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02eac763",
   "metadata": {},
   "source": [
    "# why need to combine prompt and code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1b828d",
   "metadata": {},
   "source": [
    "Great question 👌 — let’s unpack that.\n",
    "\n",
    "We **combine prompt + code into one sequence** during supervised fine-tuning because:\n",
    "\n",
    "### 1. Causal LM training objective\n",
    "\n",
    "* StarCoder and similar models are **causal language models** (predict the next token given all previous ones).\n",
    "* They don’t have a special “input vs label” separation like an encoder–decoder (T5, BART).\n",
    "* To make them condition on a prompt, we concatenate:\n",
    "\n",
    "```\n",
    "[prompt tokens] [code tokens]\n",
    "```\n",
    "\n",
    "and then tell the model:\n",
    "\n",
    "* **Predict the code tokens**,\n",
    "* **Ignore the prompt tokens in the loss** (mask them with `-100`).\n",
    "\n",
    "So the model sees the prompt in its context and learns: *“after this kind of prompt, the next tokens should be this code.”*\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Why not train only on code?\n",
    "\n",
    "If you fed only the code as the target sequence, the model wouldn’t learn the mapping from **natural-language prompt → code**.\n",
    "It would just learn to continue random code snippets, like pretraining.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Flexibility for inference\n",
    "\n",
    "At inference time, you give only the prompt:\n",
    "\n",
    "```\n",
    "[prompt tokens]\n",
    "```\n",
    "\n",
    "and the model will auto-regressively generate the continuation:\n",
    "\n",
    "```\n",
    "[code tokens]\n",
    "```\n",
    "\n",
    "Because that’s what it was trained to do.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Analogy with instruction tuning\n",
    "\n",
    "This is the same trick instruction-tuned LLMs use.\n",
    "E.g. training examples are formatted like:\n",
    "\n",
    "```\n",
    "### Instruction:\n",
    "Write a Python function that reverses a string.\n",
    "\n",
    "### Response:\n",
    "def reverse_string(s):\n",
    "    return s[::-1]\n",
    "```\n",
    "\n",
    "The model sees the instruction + response together, but the loss is only on the response tokens.\n",
    "\n",
    "---\n",
    "\n",
    "⚡So in short:\n",
    "We combine `prompt + code` into one sequence **because causal LMs only know “next token prediction,” and this setup forces them to learn: given this prompt, continue with this code.**\n",
    "\n",
    "---\n",
    "\n",
    "Do you want me to also show you what the **dataset entries should look like** before and after tokenization, so you see exactly how the loss masking works?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8634294e",
   "metadata": {},
   "source": [
    "# Type of fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2daf95e8",
   "metadata": {},
   "source": [
    "Yes ✅ — the script I gave you is **supervised fine-tuning (SFT)** with QLoRA.\n",
    "\n",
    "Here’s why:\n",
    "\n",
    "* You already have pairs of **(prompt → code)** in your dataset.\n",
    "* We tokenize both, then mask the **prompt tokens with `-100`** in the labels so the loss is computed **only on the code part**.\n",
    "* During training, the model is directly optimized with **cross-entropy loss** to predict the ground-truth code given the prompt.\n",
    "\n",
    "That’s exactly what supervised fine-tuning means: you have gold outputs (your code) for each input (prompt), and you minimize the loss between model predictions and these targets.\n",
    "\n",
    "👉 Later, if you want to go beyond SFT:\n",
    "\n",
    "* **RLHF / DPO**: Add preference data to align the model with what’s “better” code.\n",
    "* **Self-Refinement / Feedback**: Use syntax/compilation/IOU feedback loops (like you mentioned for CAD codegen).\n",
    "* **Unsupervised adaptation**: Only next-token training on raw code (like continuing pretraining).\n",
    "\n",
    "Do you want me to also show you how to set this up in a **reinforcement learning loop** (e.g., DPO or RLHF) after SFT? That’s often the next step for codegen models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db94585",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e962610",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
