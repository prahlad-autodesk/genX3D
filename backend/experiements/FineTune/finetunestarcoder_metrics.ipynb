{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af0d547e",
   "metadata": {},
   "source": [
    "Excellent question üöÄ ‚Äî evaluation in **code generation** (like fine-tuning **StarCoder**) is **very different** from image classification with ViTs.\n",
    "\n",
    "Let‚Äôs go step by step üëá\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 1. Nature of Task\n",
    "\n",
    "* Dataset = **prompt ‚Üí code** (seq2seq, causal LM generation).\n",
    "* Output = **source code string**.\n",
    "* Unlike classification (accuracy), we need metrics that capture **semantic correctness** and **syntactic quality**.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 2. Common Evaluation Metrics for Code Generation\n",
    "\n",
    "### ‚úÖ **Perplexity (PPL)**\n",
    "\n",
    "* Measures how well the model predicts the next token.\n",
    "* Lower perplexity = better language modeling.\n",
    "* Good for **training evaluation**, but doesn‚Äôt guarantee correctness of generated code.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **BLEU / ROUGE**\n",
    "\n",
    "* Compare n-gram overlap between generated code and reference code.\n",
    "* Example: BLEU-4 is common.\n",
    "* Issue ‚Üí Code can be semantically identical with **different variable names/formatting**, so BLEU is not always reliable.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **CodeBLEU**\n",
    "\n",
    "* Extension of BLEU designed specifically for code:\n",
    "\n",
    "  * N-gram match (like BLEU)\n",
    "  * Weighted by **syntax & structure** (AST tree similarity, dataflow similarity)\n",
    "  * More meaningful than plain BLEU.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **Exact Match (EM)**\n",
    "\n",
    "* % of predictions where generated code matches the reference **exactly**.\n",
    "* Too strict for real-world code (different formatting still works).\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **Execution-based Metrics**\n",
    "\n",
    "* Run the generated code and check correctness:\n",
    "\n",
    "  * **Pass\\@k** (from Codex/OpenAI paper):\n",
    "\n",
    "    * Generate *k* samples per prompt.\n",
    "    * Check if at least 1 sample passes all test cases.\n",
    "    * Pass\\@1, Pass\\@5, Pass\\@10 are standard.\n",
    "  * **Unit test success rate**:\n",
    "\n",
    "    * If dataset provides test cases, run the generated code and compute % passed.\n",
    "\n",
    "This is the **gold standard** for evaluating code generation, since ultimately we care about **working code**.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 3. Recommended Setup for StarCoder Fine-tuning\n",
    "\n",
    "1. **Training-time metric**:\n",
    "\n",
    "   * **Perplexity** (to monitor loss).\n",
    "\n",
    "2. **Static text-based metrics**:\n",
    "\n",
    "   * **CodeBLEU** (primary),\n",
    "   * **Exact Match** (secondary).\n",
    "\n",
    "3. **Execution-based metrics**:\n",
    "\n",
    "   * **Pass\\@k** (if you can run code safely with sandboxing).\n",
    "   * **Test case accuracy** (if your dataset includes unit tests).\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 4. Example in Practice\n",
    "\n",
    "* Suppose you fine-tuned StarCoder on a dataset of `prompt ‚Üí code`.\n",
    "* At evaluation time:\n",
    "\n",
    "  * Compute **perplexity** on held-out set.\n",
    "  * Compute **CodeBLEU** between generated vs reference code.\n",
    "  * Optionally, run generated code with test cases ‚Üí compute **Pass\\@1, Pass\\@5**.\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ So summary:\n",
    "\n",
    "* **During training** ‚Üí Loss + Perplexity\n",
    "* **After training** ‚Üí CodeBLEU, Exact Match, Execution metrics (Pass\\@k, test success rate)\n",
    "\n",
    "---\n",
    "\n",
    "üëâ Do you want me to draft a **PyTorch/Hugging Face evaluation script** that shows how to compute **perplexity + CodeBLEU + Pass\\@k** for a StarCoder fine-tuning run?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81ff5a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175a338f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# -----------------------\n",
    "# 1. Load Model & Tokenizer\n",
    "# -----------------------\n",
    "model_name = \"bigcode/starcoder\"   # replace with your fine-tuned checkpoint\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# -----------------------\n",
    "# 2. Load Dataset (prompt -> code)\n",
    "# -----------------------\n",
    "dataset = load_dataset(\"your_dataset_name\")  # replace with your dataset\n",
    "eval_data = dataset[\"test\"].select(range(100))  # small subset for demo\n",
    "\n",
    "# -----------------------\n",
    "# 3. Perplexity\n",
    "# -----------------------\n",
    "def compute_perplexity(eval_data):\n",
    "    encodings = tokenizer(\"\\n\\n\".join(eval_data[\"code\"]), return_tensors=\"pt\")\n",
    "    max_length = model.config.n_positions\n",
    "    stride = 512\n",
    "\n",
    "    lls = []\n",
    "    for i in range(0, encodings.input_ids.size(1), stride):\n",
    "        begin_loc = max(i + stride - max_length, 0)\n",
    "        end_loc = min(i + stride, encodings.input_ids.size(1))\n",
    "        trg_len = end_loc - i\n",
    "        input_ids = encodings.input_ids[:, begin_loc:end_loc].to(model.device)\n",
    "        target_ids = input_ids.clone()\n",
    "        target_ids[:, :-trg_len] = -100\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, labels=target_ids)\n",
    "            log_likelihood = outputs.loss * trg_len\n",
    "        lls.append(log_likelihood)\n",
    "\n",
    "    ppl = torch.exp(torch.stack(lls).sum() / end_loc)\n",
    "    return ppl.item()\n",
    "\n",
    "ppl = compute_perplexity(eval_data)\n",
    "print(f\"Perplexity: {ppl:.2f}\")\n",
    "\n",
    "# -----------------------\n",
    "# 4. CodeBLEU\n",
    "# -----------------------\n",
    "codebleu = evaluate.load(\"codebleu\")\n",
    "\n",
    "preds = []\n",
    "refs = []\n",
    "\n",
    "for example in eval_data:\n",
    "    inputs = tokenizer(example[\"prompt\"], return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,\n",
    "            temperature=0.2,\n",
    "            do_sample=False\n",
    "        )\n",
    "    gen_code = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    preds.append(gen_code)\n",
    "    refs.append(example[\"code\"])\n",
    "\n",
    "results = codebleu.compute(predictions=preds, references=refs, lang=\"python\")  # adjust language\n",
    "print(\"CodeBLEU:\", results)\n",
    "\n",
    "# -----------------------\n",
    "# 5. Pass@k (Execution-based, Optional)\n",
    "# -----------------------\n",
    "def compute_pass_at_k(preds, refs, k=1):\n",
    "    # Dummy version: exact string match (replace with test execution)\n",
    "    correct = 0\n",
    "    for p, r in zip(preds, refs):\n",
    "        if p.strip() == r.strip():\n",
    "            correct += 1\n",
    "    return {\"pass@{}\".format(k): correct / len(refs)}\n",
    "\n",
    "passk = compute_pass_at_k(preds, refs, k=1)\n",
    "print(passk)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c5314c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27577aaa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd139f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers datasets accelerate bitsandbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee593625",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# -----------------------\n",
    "# 1. Load Dataset\n",
    "# -----------------------\n",
    "# Your dataset must have \"prompt\" and \"code\" fields\n",
    "dataset = load_dataset(\"json\", data_files={\n",
    "    \"train\": \"train.jsonl\",\n",
    "    \"test\": \"test.jsonl\"\n",
    "})\n",
    "\n",
    "# Example record in train.jsonl:\n",
    "# {\"prompt\": \"Write a Python function to add two numbers\", \"code\": \"def add(a, b):\\n    return a+b\"}\n",
    "\n",
    "# -----------------------\n",
    "# 2. Load Model & Tokenizer\n",
    "# -----------------------\n",
    "model_name = \"bigcode/starcoder\"   # or \"bigcode/starcoderbase\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "\n",
    "# -----------------------\n",
    "# 3. Preprocess Function\n",
    "# -----------------------\n",
    "def preprocess(example):\n",
    "    # Concatenate prompt and code into a single sequence\n",
    "    # You can use a separator if needed\n",
    "    text = f\"Prompt: {example['prompt']}\\nCode:\\n{example['code']}\"\n",
    "    tokenized = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512  # adjust depending on GPU memory\n",
    "    )\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    return tokenized\n",
    "\n",
    "tokenized_ds = dataset.map(preprocess, batched=False, remove_columns=dataset[\"train\"].column_names)\n",
    "\n",
    "# -----------------------\n",
    "# 4. Training Arguments\n",
    "# -----------------------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./starcoder-finetuned\",\n",
    "    evaluation_strategy=\"epoch\",     # evaluate after each epoch\n",
    "    save_strategy=\"epoch\",           # save checkpoints each epoch\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=2,   # adjust for GPU memory\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=100,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=20,\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\",  # disable wandb unless you want logging\n",
    "    fp16=True,         # use mixed precision if on GPU\n",
    "    push_to_hub=False,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# 5. Trainer\n",
    "# -----------------------\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"test\"],\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# 6. Train\n",
    "# -----------------------\n",
    "trainer.train()\n",
    "\n",
    "# -----------------------\n",
    "# 7. Save Final Model\n",
    "# -----------------------\n",
    "trainer.save_model(\"./starcoder-finetuned\")\n",
    "tokenizer.save_pretrained(\"./starcoder-finetuned\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24644293",
   "metadata": {},
   "source": [
    "Perfect ‚ö° let‚Äôs put everything together and write a **training script to fine-tune StarCoder** on your dataset of\n",
    "\n",
    "```\n",
    "prompt  ‚Üí  code\n",
    "```\n",
    "\n",
    "pairs.\n",
    "We‚Äôll use Hugging Face **`transformers` + `datasets`** since they make this much easier.\n",
    "\n",
    "---\n",
    "\n",
    "# üîπ 1. Install Dependencies\n",
    "\n",
    "```bash\n",
    "pip install transformers datasets accelerate bitsandbytes\n",
    "```\n",
    "\n",
    "*(use `bitsandbytes` if you want to train with 8-bit optimizers on a single GPU)*\n",
    "\n",
    "---\n",
    "\n",
    "# üîπ 2. Training Script (PyTorch + Hugging Face)\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# -----------------------\n",
    "# 1. Load Dataset\n",
    "# -----------------------\n",
    "# Your dataset must have \"prompt\" and \"code\" fields\n",
    "dataset = load_dataset(\"json\", data_files={\n",
    "    \"train\": \"train.jsonl\",\n",
    "    \"test\": \"test.jsonl\"\n",
    "})\n",
    "\n",
    "# Example record in train.jsonl:\n",
    "# {\"prompt\": \"Write a Python function to add two numbers\", \"code\": \"def add(a, b):\\n    return a+b\"}\n",
    "\n",
    "# -----------------------\n",
    "# 2. Load Model & Tokenizer\n",
    "# -----------------------\n",
    "model_name = \"bigcode/starcoder\"   # or \"bigcode/starcoderbase\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "\n",
    "# -----------------------\n",
    "# 3. Preprocess Function\n",
    "# -----------------------\n",
    "def preprocess(example):\n",
    "    # Concatenate prompt and code into a single sequence\n",
    "    # You can use a separator if needed\n",
    "    text = f\"Prompt: {example['prompt']}\\nCode:\\n{example['code']}\"\n",
    "    tokenized = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512  # adjust depending on GPU memory\n",
    "    )\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    return tokenized\n",
    "\n",
    "tokenized_ds = dataset.map(preprocess, batched=False, remove_columns=dataset[\"train\"].column_names)\n",
    "\n",
    "# -----------------------\n",
    "# 4. Training Arguments\n",
    "# -----------------------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./starcoder-finetuned\",\n",
    "    evaluation_strategy=\"epoch\",     # evaluate after each epoch\n",
    "    save_strategy=\"epoch\",           # save checkpoints each epoch\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=2,   # adjust for GPU memory\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=100,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=20,\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\",  # disable wandb unless you want logging\n",
    "    fp16=True,         # use mixed precision if on GPU\n",
    "    push_to_hub=False,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# 5. Trainer\n",
    "# -----------------------\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"test\"],\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# 6. Train\n",
    "# -----------------------\n",
    "trainer.train()\n",
    "\n",
    "# -----------------------\n",
    "# 7. Save Final Model\n",
    "# -----------------------\n",
    "trainer.save_model(\"./starcoder-finetuned\")\n",
    "tokenizer.save_pretrained(\"./starcoder-finetuned\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# üîπ 3. Dataset Format\n",
    "\n",
    "Your dataset should look like this (`train.jsonl`):\n",
    "\n",
    "```json\n",
    "{\"prompt\": \"Write a Python function to compute factorial\", \"code\": \"def factorial(n):\\n    return 1 if n==0 else n*factorial(n-1)\"}\n",
    "{\"prompt\": \"Generate SQL query to select all users older than 30\", \"code\": \"SELECT * FROM users WHERE age > 30;\"}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# üîπ 4. What This Does\n",
    "\n",
    "* Concatenates **prompt + code** into a single training example.\n",
    "* Trains StarCoder in **causal LM mode** (predict next token).\n",
    "* Loss = **CrossEntropyLoss** on next-token prediction.\n",
    "* Optimizer = **AdamW** (default in Hugging Face `Trainer`).\n",
    "* You can evaluate using **perplexity** + custom metrics (CodeBLEU, Pass\\@k).\n",
    "\n",
    "---\n",
    "\n",
    "üëâ Do you want me to also add a **LoRA/PEFT version** of this (parameter-efficient fine-tuning), which is much cheaper and faster if you don‚Äôt want to fine-tune the full StarCoder (\\~15B parameters)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febe0f7a",
   "metadata": {},
   "source": [
    "Great question ‚ö° ‚Äî let‚Äôs walk through what happens **after fine-tuning StarCoder** on your `prompt ‚Üí code` dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 1. What the Model Learned\n",
    "\n",
    "During fine-tuning:\n",
    "\n",
    "* The model saw examples like:\n",
    "\n",
    "```\n",
    "Prompt: Write a Python function to compute factorial\n",
    "Code:\n",
    "def factorial(n):\n",
    "    return 1 if n==0 else n*factorial(n-1)\n",
    "```\n",
    "\n",
    "* So it learns:\n",
    "\n",
    "  * Given a **prompt**, it should continue generating with the **code**.\n",
    "  * Essentially, the model is trained as a **prompt ‚Üí completion** generator.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 2. Inference (After Training)\n",
    "\n",
    "At inference time, when a user asks something like:\n",
    "\n",
    "```python\n",
    "prompt = \"Write a Python function to reverse a string\"\n",
    "```\n",
    "\n",
    "We tokenize and pass it to the model:\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_path = \"./starcoder-finetuned\"  # your saved checkpoint\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "inputs = tokenizer(f\"Prompt: {prompt}\\nCode:\\n\", return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=200,\n",
    "        temperature=0.2,   # lower for deterministic, higher for diversity\n",
    "        top_p=0.95,\n",
    "        do_sample=True\n",
    "    )\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 3. Example Output\n",
    "\n",
    "For the above prompt, the model might generate something like:\n",
    "\n",
    "```python\n",
    "Prompt: Write a Python function to reverse a string\n",
    "Code:\n",
    "def reverse_string(s):\n",
    "    return s[::-1]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 4. Key Points\n",
    "\n",
    "* The **output is just text** (string containing code).\n",
    "* The quality depends heavily on:\n",
    "\n",
    "  * Size/quality of your fine-tuning dataset.\n",
    "  * Whether prompts were consistent (`Prompt: ... Code:` style).\n",
    "* If you trained with **only the code (no structured ‚ÄúPrompt: ‚Ä¶ Code:‚Äù format)**, the model may just generate raw code completions.\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ So after fine-tuning:\n",
    "\n",
    "* **Input** = natural language prompt (e.g., ‚ÄúWrite SQL query for ‚Ä¶‚Äù)\n",
    "* **Output** = model-generated code (Python, SQL, etc., depending on your dataset).\n",
    "\n",
    "---\n",
    "\n",
    "üëâ Do you want me to show you how to wrap this into a **Flask/FastAPI inference server**, so users can hit an endpoint with a prompt and get back generated code?\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
